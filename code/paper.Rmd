---
title: "Community Data Science meets Digital Heritage: Controversies about World Heritage Sites in the Online Community of Wikipedia"
---

Load the data

```{r}
library(tidyverse)

# load the random pages
ten_k_random_wp_pages <- readRDS(here::here("data", "random_page_data_tbl.rds"))

# load the random pages revision histories
ten_k_random_wp_pages_lst <- readRDS(here::here("data", "random_page_data_lst.rds"))

# all WH sites with WP pages (n = 584)
page_data_for_all_pages <- 
  readRDS(here::here("data", 
                     "page_data_for_all_pages.rds"))

# all WH sites, with and without WP pages (n = 846)
wh_wiki_table <- 
  read_csv(here::here("data", 
                      "wh_wiki_table.csv"))

# all WH sites according to UNESCO
# from https://whc.unesco.org/en/syndication
wh_unesco <- readxl::read_excel(here::here("data",
                                           "whc-sites-2018.xls")) %>% 
  filter(category %in% c("Cultural", "Mixed"))

library("rnaturalearth")
library("rnaturalearthdata")
world <- ne_countries(scale = "medium", returnclass = "sf")

# functions used more than once:

estimate_mode <- function(x) {
  d <- density(x)
  d$x[which.max(d$y)]
}

theme_nogrid <- function (base_size = 12, base_family = "") {
  theme_bw(base_size = base_size, 
           base_family = base_family) %+replace% 
    theme(panel.grid = element_blank() )   
}
```

## Representation by country

Where are the WH sites, before looking into WP

```{r}

wh_unesco_countries <- 
wh_unesco %>% 
  separate_rows(states_name_en, convert = TRUE, sep = ",") %>% 
  mutate(country = case_when(
    states_name_en == "Bolivia (Plurinational State of)"  ~ "Bolivia",
    states_name_en == "Cabo Verde"  ~ "Republic of Cabo Verde",
    states_name_en == "Czechia"  ~ "Czech Republic",
    states_name_en == "Democratic People's Republic of Korea"  ~ "Republic of Korea",
    states_name_en == "Gambia (the)"  ~ "The Gambia",
    states_name_en == "Holy See"  ~ "Vatican",
    states_name_en == "Iran (Islamic Republic of)"  ~ "Iran",
    states_name_en == "Lao People's Democratic Republic"  ~ "Lao PDR",
    states_name_en == "Micronesia (Federated States of)"  ~ "Federated States of Micronesia",
    states_name_en == "Syrian Arab Republic"  ~ "Syria",
    states_name_en == "the former Yugoslav Republic of Macedonia"  ~ "Macedonia",
    states_name_en == "United Kingdom of Great Britain and Northern Ireland"  ~ "United Kingdom",
    states_name_en == "United Republic of Tanzania"  ~ "Tanzania",
    states_name_en == "United States of America"  ~ "United States",
    states_name_en == "Venezuela (Bolivarian Republic of)"  ~ "Venezuela",
    states_name_en == "United States of America"  ~ "United States",
    states_name_en == "Viet Nam"  ~ "Vietnam",
    states_name_en == "Republic of Moldova"  ~ "Moldova",
    TRUE ~ as.character(states_name_en)
  ))

wh_unesco_countries_count <-  
  wh_unesco_countries %>% 
  count(country)

median_number_unesco_sites_per_country <- 
  median(wh_unesco_countries_count$n)

mode_number_unesco_sites_per_country <- 
  round(estimate_mode(wh_unesco_countries_count$n), 1)

wh_unesco_country_counts_hist <- 
ggplot(wh_unesco_countries_count,
       aes(n)) +
  geom_histogram() +
  annotate("text", x = 30, y = 30, 
           label = str_glue('Median = {median_number_unesco_sites_per_country} WH sites/country\nMode = {mode_number_unesco_sites_per_country} WH sites/country'),
           size = 3) +
  theme_nogrid(12)

sf_map_data_unesco <- 
world %>% 
  left_join(wh_unesco_countries_count,
            by = c( 'name_long' = 'country')) %>% 
  select(name, n, geometry) %>% 
  mutate(n = ifelse(is.na(n), 0, n)) %>% 
  filter(name != "Antartica")

wh_country_unesco_count_map <- 
ggplot(data = sf_map_data_unesco) +
    geom_sf(aes(fill = n), lwd = 0) +
    scale_fill_viridis_c(na.value="grey90") +
  coord_sf() +
  coord_sf(ylim = c(-50, 90), datum = NA) +
  theme_minimal()

wh_country_unesco_count_map +
  annotation_custom(ggplotGrob(wh_unesco_country_counts_hist), 
                    xmin = -190, 
                    xmax = -90, 
                    ymin= 10, 
                    ymax=-70) 

```

What countries are most represented?

```{r}
  # get country of site from location text
  # Laos, Czech Republic, Micronesia, Zimbabwe, South Sudan, Chad, 
  # Central African Republic, Congo, Gabon, Cameroon, Nigeria, Bosnia and Herzegovina
  #  Cote d'Ivoire, Sierra Leone, Guyana, Belize, 
  country_names <-  paste(c(world$name, 
                            "United States",
                            "Czech Republic",
                             "Antigua & Barbuda",
                            "Saint Kitts and Nevis"
                            ),
                          collapse="|")

  page_data_for_all_pages_location <- 
    page_data_for_all_pages %>% 
    mutate(country = str_extract(Location, 
                                 regex(country_names, 
                                       ignore.case=TRUE))) %>% 
    mutate(country = case_when(
      country == "United States" ~ "United States of America",
      country == "Czech Republic" ~ "Czechia",
      country == "Antigua & Barbuda" ~ "Antigua and Barb.",
      country == "Saint Kitts and Nevis" ~ "St. Kitts and Nevis",
      TRUE ~ as.character(country)))
  
  
wh_wiki_table_location <- 
  wh_wiki_table %>% 
  mutate(country = str_extract(Location, 
                                 regex(country_names, 
                                       ignore.case=TRUE))) %>% 
    mutate(country = case_when(
      country == "United States" ~ "United States of America",
      country == "Czech Republic" ~ "Czechia",
      country == "Antigua & Barbuda" ~ "Antigua and Barb.",
      country == "Saint Kitts and Nevis" ~ "St. Kitts and Nevis",
      TRUE ~ as.character(country)))
  
```


```{r}

wh_wk_country_counts <- 
page_data_for_all_pages_location %>% 
  count(country) %>% 
  filter(!is.na(country))

theme_nogrid <- function (base_size = 12, base_family = "") {
  theme_bw(base_size = base_size, 
           base_family = base_family) %+replace% 
    theme(panel.grid = element_blank() )   
}

median_number_wp_pages_per_country <- 
  median(wh_wk_country_counts$n)

estimate_mode <- function(x) {
  d <- density(x)
  d$x[which.max(d$y)]
}

mode_number_wp_pages_per_country <- 
  round(estimate_mode(wh_wk_country_counts$n), 1)

wh_wk_country_counts_hist <- 
ggplot(wh_wk_country_counts,
       aes(n)) +
  geom_histogram() +
  annotate("text", x = 15, y = 30, 
           label = str_glue('Median = {median_number_wp_pages_per_country} WP pages/country\nMode = {mode_number_wp_pages_per_country} WP pages/country'),
           size = 2) +
  theme_nogrid(12)

wh_wk_country_counts_bar <- 
ggplot(wh_wk_country_counts,
       aes(reorder(country, n),
           n)) +
  geom_col() +
  coord_flip() +
  theme_minimal(base_size = 6) +
  xlab("")
```

Here's a map of countries showing which has the most WP pages for WH sites

```{r}
sf_map_data <- 
world %>% 
  left_join(wh_wk_country_counts,
            by = c( 'name' = 'country')) %>% 
  select(name, n, geometry) %>% 
  mutate(n = ifelse(is.na(n), 0, n)) %>% 
  filter(name != "Antartica")

wh_wk_country_count_map <- 
ggplot(data = sf_map_data) +
    geom_sf(aes(fill = n), lwd = 0) +
    scale_fill_viridis_c(na.value="grey90") +
  coord_sf() +
  coord_sf(ylim = c(-50, 90), datum = NA) +
  theme_minimal()

wh_wk_country_count_map +
  annotation_custom(ggplotGrob(wh_wk_country_counts_hist), 
                    xmin = -190, 
                    xmax = -90, 
                    ymin= 10, 
                    ymax=-70) 
```

Map of the ratio of WH sites with WP pages, per country

```{r}
wh_wiki_table_location_prop <- 
wh_wiki_table_location %>% 
  mutate(has_wp_page = ifelse(!is.na(site_page_name), 1, 0)) %>% 
  group_by(country) %>% 
  summarise(prop_sites_with_pages = sum(has_wp_page) / n()) %>% 
  filter(!is.na(country))

wh_wk_country_prop_hist <- 
ggplot(wh_wiki_table_location_prop,
       aes(prop_sites_with_pages)) +
  geom_histogram() +
  theme_nogrid(10)

wh_wk_country_all_counts <- 
wh_wiki_table_location %>% 
  count(country) %>% 
  filter(!is.na(country)) %>% 
  left_join(wh_wiki_table_location_prop)

wh_wk_country_prop_scatter <- 
ggplot(wh_wk_country_all_counts,
       aes(prop_sites_with_pages, n)) +
  geom_point(alpha = 0.5) +
  coord_fixed(0.02) +
  theme_nogrid(6)

wh_wk_country_prop_scatter_marginal <- 
ggExtra::ggMarginal(wh_wk_country_prop_scatter,
                    type = "histogram")

sf_map_data_prop <- 
world %>% 
  filter(name != "Antartica") %>% 
  left_join(wh_wiki_table_location_prop,
            by = c( 'name' = 'country')) %>% 
  select(name, 
         prop_sites_with_pages, 
         geometry)  %>% 
  mutate(prop_sites_with_pages = ifelse(is.na(prop_sites_with_pages), 0, prop_sites_with_pages))

wh_wk_country_prop_map <- 
ggplot(data = sf_map_data_prop) +
    geom_sf(aes(fill = prop_sites_with_pages), lwd = 0) +
    scale_fill_viridis_c(na.value="grey90") +
  coord_sf() +
  coord_sf(ylim = c(-50, 90), datum = NA) +
  theme_minimal()

wh_wk_country_prop_map +
  annotation_custom((wh_wk_country_prop_scatter_marginal), 
                    xmin = -190, 
                    xmax = -90, 
                    ymin= 10, 
                    ymax=-70) 
```


## Basic qualities of the WP articles about WH sites

```{r}
ten_k_random_wp_pages_basic_qualities <- 
ten_k_random_wp_pages %>% 
  select(page_wordcount,
         page_wikilinks_out, 
         page_cited_items_on) %>% 
  mutate(rnd_page_wikilinks_out_per_1k_words = page_wikilinks_out / page_wordcount * 1000,
         rnd_page_cited_items_on_per_1k_words = page_cited_items_on / page_wordcount * 1000,
         rnd_page_wordcount = page_wordcount) %>% 
  select(-page_wikilinks_out,
         -page_cited_items_on,
         -page_wordcount) 

ten_k_random_wp_pages_basic_qualities_long <- 
  ten_k_random_wp_pages_basic_qualities %>% 
  gather(variable, value) 
```


Length: correlate with ??
number of links out/word
number of references/word: scholarly or not

```{r}
page_data_for_all_pages_basic_qualities <- 
page_data_for_all_pages %>% 
  select(page_wordcount,
         page_wikilinks_out, 
         page_cited_items_on) %>% 
  mutate(page_wikilinks_out_per_1k_words = page_wikilinks_out / page_wordcount * 1000,
         page_cited_items_on_per_1k_words = page_cited_items_on / page_wordcount * 1000) %>% 
  select(-page_wikilinks_out,
         -page_cited_items_on)

page_data_for_all_pages_basic_qualities_long <- 
page_data_for_all_pages_basic_qualities %>% 
  gather(variable, value) %>% 
  bind_rows(ten_k_random_wp_pages_basic_qualities_long) %>% 
  mutate(source = ifelse(str_detect(variable, "rnd" ), "10k Random pages", "World Heritage pages")) %>% 
  mutate(variable = str_remove(variable, "rnd_"))

# density plots
  ggplot(page_data_for_all_pages_basic_qualities_long,
         aes(value)) +
  geom_density(aes(fill = source),
               alpha = 0.3) +
  scale_x_log10() +
  scale_fill_viridis_d() +
  facet_wrap( ~ variable, 
              scales = "free") +
  theme_minimal()
```

```{r}
# scatterplots
  ggplot() +
      geom_point(data = ten_k_random_wp_pages_basic_qualities,
             aes(rnd_page_wikilinks_out_per_1k_words,
                 rnd_page_cited_items_on_per_1k_words,
                 size = rnd_page_wordcount),
             colour = "grey80",
             alpha = 0.4) +
  geom_point(data = page_data_for_all_pages_basic_qualities,
             aes(page_wikilinks_out_per_1k_words,
                 page_cited_items_on_per_1k_words,
                 size = page_wordcount),
             colour = "red",
             alpha = 0.8) +
  scale_x_log10() +
  scale_y_log10() +
  scale_fill_viridis_d() +
  coord_fixed() +
  theme_minimal()

```

## Basic qualitites of attention paid to articles

number of links in
page_views_last_n_days_total

```{r}
ten_k_random_wp_pages_attent <- 
ten_k_random_wp_pages %>% 
  select(page_wikilinks_in, 
         page_views_last_n_days_total) 

page_data_for_all_pages_attent <- 
page_data_for_all_pages %>% 
  select(page_wikilinks_in, 
         page_views_last_n_days_total) 

page_data_for_all_pages %>% 
  select( Site,
         page_wikilinks_in, 
         page_views_last_n_days_total)  %>% 
  ggplot(aes(page_wikilinks_in,
             page_views_last_n_days_total,
             label = Site)) +
  geom_point(alpha = 0.4) +
    geom_text_repel(
    data          = page_data_for_all_pages %>%  
                      filter(page_views_last_n_days_total > 190000),
    segment.size  = 0.2,
    segment.color = "grey50",
    direction     = "both"
  ) +
  scale_y_log10(expand = c(1, 0.05)) +
  scale_x_log10(limits = c(1, 10000)) +
  coord_fixed(0.5) +
  theme_minimal() 

page_data_for_all_pages_attent_long <- 
page_data_for_all_pages_attent %>% 
  gather(variable, value) %>% 
  bind_rows(ten_k_random_wp_pages_attent %>% 
              gather(variable, value) %>% 
              mutate(variable =  str_glue('rnd_{variable}'))) %>% 
  mutate(source = ifelse(str_detect(variable, "rnd" ), "10k Random pages", 
                         "World Heritage pages")) %>% 
  mutate(variable = str_remove(variable, "rnd_"))

# density plots
  ggplot(page_data_for_all_pages_attent_long,
         aes(value)) +
  geom_density(aes(fill = source),
               alpha = 0.3) +
  scale_x_log10() +
  scale_fill_viridis_d() +
  facet_wrap( ~ variable, 
              scales = "free") +
  theme_minimal()
```

## Basic qualities of the editing process

number of edits, total, prop/words
size of edits
divesity of editors

```{r}
revision_history_page_details <- 
  tibble(revision_history_page_details = map(page_data_for_all_pages$page_info_t, 
      ~.x$revision_history_page_details)) %>% 
  mutate(Site =         page_data_for_all_pages$Site,
         Country =      page_data_for_all_pages$country,
         rh_n_editors = map_int(revision_history_page_details, ~n_distinct(.x$rh_user)),
         rh_n_edits =   map_int(revision_history_page_details, ~nrow(.x)),
         rh_user_simpson_idx = page_data_for_all_pages$rh_user_simpson_idx,
         rh_user_bot_prop = page_data_for_all_pages$rh_user_bot_prop,
         rh_revert_prop = page_data_for_all_pages$rh_revert_prop)

ggplot(revision_history_page_details,
       aes(rh_n_edits)) +
  geom_histogram()

```


## Locality  

IP addresses

```{r}

ip_regex <- "(?(?=.*?(\\d+\\.\\d+\\.\\d+\\.\\d+).*?)(\\1|))"

get_ip_addresses <- function(x){
  re <- regexpr(
  ip_regex, 
  x, perl = TRUE)
regmatches(x, re)
}

# get locations from IP addresses
library(rgeolocate)
ip_db <- system.file("extdata","GeoLite2-Country.mmdb", package = "rgeolocate")

```

Do WH sites have more geographically diverse contributions?

Get locations for random pages.

How many IP edits on each page?

```{r}
randoms_revision_history_page_details <- 
  map_df(ten_k_random_wp_pages_lst, 
         ~.x$revision_history_page_details, 
         .id = "page") 

randoms_revision_history_page_details_ip <- 
randoms_revision_history_page_details %>% 
    mutate(is_ip_address = ifelse(get_ip_addresses(rh_user) != "", TRUE, FALSE),
         page = as.numeric(page)) 
  
randoms_revision_history_page_details_ip_prop <- 
  randoms_revision_history_page_details_ip %>% 
  group_by(page) %>% 
  summarise(prop_ip_addresses = sum(is_ip_address) / n() )

ggplot(randoms_revision_history_page_details_ip_prop,
       aes(prop_ip_addresses)) +
  geom_histogram() +
  theme_minimal()
```

Overall location of IP edits for all random pages

```{r}
randoms_revision_history_page_details_ip_location <- 
randoms_revision_history_page_details_ip %>% 
  filter(is_ip_address) %>% 
  mutate(ip_location = map(rh_user, ~maxmind(.x, ip_db)))  %>% 
  unnest(ip_location)

randoms_revision_history_page_details_ip_location_tally <- 
randoms_revision_history_page_details_ip_location %>% 
  filter(!is.na(country_name)) %>% 
  mutate(country_name = fct_rev(fct_infreq(country_name))) %>% 
  group_by(country_name) %>% 
  tally(sort = TRUE) %>% 
  filter(n > 100) 

ggplot(randoms_revision_history_page_details_ip_location_tally,
       aes(country_name, n)) +
  geom_col() +
  coord_flip()   +
  theme_minimal() +
  scale_y_continuous(labels = scales::comma)
```

What's the geographic diversity of edits per random page?

```{r}
randoms_revision_history_page_details_ip_location_props_per_page <- 
randoms_revision_history_page_details_ip_location %>% 
  filter(!is.na(country_name)) %>% 
  mutate(country_name = fct_rev(fct_infreq(country_name))) %>% 
  group_by(page, country_name) %>% 
  tally() %>% 
  mutate(prop = n / sum(n)) %>% 
  filter(country_name %in% randoms_revision_history_page_details_ip_location_tally$country_name)

ggplot(randoms_revision_history_page_details_ip_location_props_per_page,
       aes(country_name, 
           prop)) +
  geom_boxplot() +
  coord_flip()

randoms_revision_history_page_details_ip_location_props_per_page_div <- 
  randoms_revision_history_page_details_ip_location_props_per_page %>% 
  group_by(page) %>% 
  summarise(shannon = vegan::diversity(n)) 

ggplot(randoms_revision_history_page_details_ip_location_props_per_page_div,
       aes( shannon)) +
  geom_histogram() 
```


```{r}
# analyse the countries of IP address vs country of WH site
revision_history_page_details_ip_addresses <- 
revision_history_page_details %>% 
  mutate(edits_from_ip_addresses = map(revision_history_page_details,
                          ~.x %>% 
                            filter(get_ip_addresses(rh_user) != "") %>% 
                            mutate(ip_location = map(rh_user, 
                                                     ~maxmind(.x, ip_db)))  %>% 
                            unnest(ip_location))) 
                            
revision_history_page_details_ip_addresses_df <- 
revision_history_page_details_ip_addresses %>% 
  unnest(edits_from_ip_addresses) 

revision_history_page_details_ip_addresses_countries <- 
revision_history_page_details_ip_addresses_df %>% 
  select(Site, Country, country_name, rh_n_edits ) %>% 
  filter_all(all_vars(!is.na(.))) %>% 
  group_by(Site, Country, rh_n_edits) %>% 
  count(country_name) %>% 
  mutate(prop_anon_of_all_edits = n / rh_n_edits) %>% 
  mutate(internal_edits = ifelse(Country ==  country_name, 
                                 TRUE, FALSE),
         n_anon_edits_total = n) %>% 
  left_join(revision_history_page_details_ip_addresses_df) %>% 
  distinct(Site,  
           Country, 
           prop_anon_of_all_edits, 
           internal_edits,
           .keep_all = TRUE) %>% 
  group_by(Site,  Country) %>% 
  summarise(sum_prop_internal = sum(n_anon_edits_total[internal_edits]) / sum(n_anon_edits_total),
            sum_prop_external = 1 - sum_prop_internal,
            rh_n_edits = unique(rh_n_edits),
            rh_n_edits_anon = sum(n_anon_edits_total),
            prop_anon_of_all_edits = sum(n) / rh_n_edits)
```


```{r}
# Histogram of proportion of anonymous edits 
# TODO and random sites
ggplot(revision_history_page_details_ip_addresses_countries,
       aes(prop_anon_of_all_edits)) +
  geom_histogram() +
  theme_minimal()
```

```{r}
# Histogram of proportion of all anonymous edits that come from same country as WH site
ggplot(revision_history_page_details_ip_addresses_countries,
       aes(sum_prop_internal)) +
  geom_histogram() +
  theme_minimal() 
```


```{r}
ggplot(revision_history_page_details_ip_addresses_countries,
       aes(sum_prop_internal,
           prop_anon_of_all_edits)) +
  geom_point() +
  theme_minimal() +
  coord_equal()

```

```{r}
adjacency_list_3_cols <- 
revision_history_page_details_ip_addresses_df %>% 
  filter(!is.na(Country),
         !is.na(country_name)) %>% 
  group_by(Country, country_name) %>% 
  count() %>% 
  rename(to = Country,
         from = country_name,
         value = n) %>% 
  ungroup() %>% 
  mutate(from = ifelse(from == "Hashemite Kingdom of Jordan", "Jordan", from)) %>% 
  select(from, to , value) %>% 
  filter(value > 100) %>% 
  left_join( revision_history_page_details_ip_addresses_df %>% 
               select(country_name, continent_name) %>% 
               distinct(),
             by = c('from' = 'country_name')) %>% 
  arrange(continent_name,  from, to) 


adjacency_list <- 
adjacency_list_3_cols%>% 
  select(-continent_name)

get_countries_in_continent <- function(x) {
  
  xx <- 
  revision_history_page_details_ip_addresses_df %>% 
    select(country_name, continent_name) %>% 
    filter(continent_name == x) %>% 
    pull(country_name) %>% 
    unique()
  
  
  xx[
 unique( c(adjacency_list$from,
           adjacency_list$to))
 ]
}

africa   <- get_countries_in_continent("Africa")        
asia    <- get_countries_in_continent("Asia")       
europe  <-get_countries_in_continent("Europe")  
north_america  <- get_countries_in_continent("North America")         
oceania <-  get_countries_in_continent("Oceania") 
south_america  <-  get_countries_in_continent("South America") 

library(circlize)
circos.clear()
chordDiagram(adjacency_list, 
             directional = 1,
             direction.type = c("diffHeight", "arrows"),
             link.arr.type = "big.arrow", 
             diffHeight = -uh(2, "mm"),
             annotationTrack = "grid", 
             preAllocateTracks = list(track.height = max(0.4)))

# we go back to the first track and customize sector labels
circos.track(track.index = 1, panel.fun = function(x, y) {
    circos.text(CELL_META$xcenter, CELL_META$ylim[1], CELL_META$sector.index, 
        facing = "clockwise", niceFacing = TRUE, adj = c(0, 0.5))
}, bg.border = NA) # here set bg.border to NA is important


highlight.sector(africa, track.index = 1, col = "#00FF0040", 
    text = "Africa", cex = 0.8, text.col = "white", niceFacing = TRUE)

highlight.sector(asia, track.index = 1, col = "#0000FF40", 
    text = "Asia", cex = 0.8, text.col = "white", niceFacing = TRUE)

highlight.sector(europe, track.index = 1, col = "blue", 
    text = "Europe", cex = 0.8, text.col = "white", niceFacing = TRUE)

highlight.sector(north_america, track.index = 1, col = "orange", 
    text = "North America", cex = 0.8, text.col = "white", niceFacing = TRUE)

highlight.sector(oceania, track.index = 1, col = "purple", 
    text = "Oceania", cex = 0.8, text.col = "white", niceFacing = TRUE)

highlight.sector(south_america, track.index = 1, col = "brown", 
    text = "South America", cex = 0.8, text.col = "white", niceFacing = TRUE)

#-- 
circos.track(track.index = 2, panel.fun = function(x, y) {
    sector.index = get.cell.meta.data("sector.index")
    xlim = get.cell.meta.data("xlim")
    ylim = get.cell.meta.data("ylim")
    circos.text(mean(xlim), mean(ylim), sector.index, cex = 0.6, niceFacing = TRUE)
}, bg.border = NA)




#--
circos.clear()
circos.par(gap.after = rep(c(rep(1, 4), 8), 3))

chordDiagram(adjacency_list, annotationTrack = c("grid", "axis"),
    preAllocateTracks = list(
        track.height = uh(4, "mm"),
        track.margin = c(uh(4, "mm"), 0)
))             
             
circos.track(track.index = 2, panel.fun = function(x, y) {
    sector.index = get.cell.meta.data("sector.index")
    xlim = get.cell.meta.data("xlim")
    ylim = get.cell.meta.data("ylim")
    circos.text(mean(xlim), mean(ylim), sector.index, cex = 0.6, niceFacing = TRUE)
}, bg.border = NA)

highlight.sector(africa, track.index = 1, col = "red", 
    text = "Africa", cex = 0.8, text.col = "white", niceFacing = TRUE)

highlight.sector(asia, track.index = 1, col = "green", 
    text = "Asia", cex = 0.8, text.col = "white", niceFacing = TRUE)

highlight.sector(europe, track.index = 1, col = "blue", 
    text = "Europe", cex = 0.8, text.col = "white", niceFacing = TRUE)

highlight.sector(north_america, track.index = 1, col = "orange", 
    text = "North America", cex = 0.8, text.col = "white", niceFacing = TRUE)

highlight.sector(oceania, track.index = 1, col = "purple", 
    text = "Oceania", cex = 0.8, text.col = "white", niceFacing = TRUE)

highlight.sector(south_america, track.index = 1, col = "brown", 
    text = "South America", cex = 0.8, text.col = "white", niceFacing = TRUE)
  

```

## Networks

editors

## Controversies

Look at distributions of variables, then say 2sd is extreme, and match those to the WH pages to find out what they are

What's the typical distribution of talk page length? 

```{r}
ten_k_random_wp_pages %>% 
  mutate(tp_divided_by_page_word_count = talk_page_wordcount / page_wordcount) %>% 
  ggplot(aes(talk_page_wordcount)) +
  geom_histogram() +
  scale_x_log10(labels = scales::comma_format(accuracy = 0.01))
```

Can we detect -ve sentiment on the talk pages? Perhaps as distributions, over every 100 words per page

```{r}
library(tidytext)

```

What's the typical distribution of the proportion of edits that are reverts?

```{r}
ggplot(ten_k_random_wp_pages,
       aes(rh_revert_prop)) +
  geom_histogram() +
  scale_x_log10()

```

What are the keywords for controversy in edit messages? And how common are these warrning messages?

```{r}
randoms_revision_history_page_details_ip_warring_words <- 
randoms_revision_history_page_details_ip %>% 
  mutate(warring_words = (str_detect(rh_comment, 'undid|vandalism|why|inaccurate|wrong|warning|blocked|warring|disputed|neutral|unacceptable' )))

# distribution of edits with warring words
randoms_revision_history_page_details_ip_warring_words_prop <- 
randoms_revision_history_page_details_ip_warring_words %>% 
  group_by(page) %>% 
  summarise(prop_warring_edits = sum(warring_words) / n()) 

ggplot(randoms_revision_history_page_details_ip_warring_words_prop,
         aes(prop_warring_edits)) +
  geom_histogram() +
  scale_x_log10()
```

Is there are correlation between the proportion of warring words and talk page length?

```{r}
# some interesting groupings
randoms_revision_history_page_details_ip_warring_words_prop %>% 
  left_join(ten_k_random_wp_pages %>% 
  mutate(page = 1:nrow(.))) %>% 
  filter(prop_warring_edits > 0) %>% 
  ggplot(aes(prop_warring_edits, 
             talk_page_wordcount ,
             size = page_wordcount)) +
  geom_point(alpha = 0.2)
```

Is there are correlation between the proportion of warring words and revert frequency?

```{r}
randoms_revision_history_page_details_ip_warring_words_prop_revert_prop <- 
randoms_revision_history_page_details_ip_warring_words_prop %>% 
  left_join(ten_k_random_wp_pages %>% 
  mutate(page = 1:nrow(.))) %>% 
  filter(prop_warring_edits != 0,
         rh_revert_prop != 0) 

lm(rh_revert_prop ~ prop_warring_edits, 
   data = randoms_revision_history_page_details_ip_warring_words_prop_revert_prop) %>% summary

ggplot(randoms_revision_history_page_details_ip_warring_words_prop_revert_prop,
         aes(prop_warring_edits, 
             rh_revert_prop)) +
  geom_point()
```

time between edits

```{r}
library(lubridate)
randoms_revision_history_page_details_ip_time_diffs <- 
randoms_revision_history_page_details_ip %>% 
  group_nest(page) %>% 
  mutate(time_diff = map(data, ~lag(.x$rh_date) - .x$rh_date)) %>% 
  unnest(time_diff) %>% 
  mutate(time_diff_days = as.numeric(time_diff / 60 / 60 / 24)) %>% 
  mutate(time_diff_hours = as.numeric(time_diff / 60 / 60 )) %>% 
  mutate(less_than_one_month = ifelse(time_diff_hours < 744, TRUE, FALSE)) %>% 
  mutate(less_than_one_week = ifelse(time_diff_hours < 168, TRUE, FALSE)) %>% 
  bind_cols(randoms_revision_history_page_details_ip)  

rle_tbl <- 
tibble(lths = rle(randoms_revision_history_page_details_ip_time_diffs$rh_user)$lengths,
vals = rle(randoms_revision_history_page_details_ip_time_diffs$rh_user)$values) 

rle_tbl2 <- tibble(
  rh_user = rep(rle_tbl$vals, rle_tbl$lths),
  lths2 = rep(rle_tbl$lths, rle_tbl$lths)
)

randoms_revision_history_page_details_ip_time_diffs %>% 
  filter(!is.na(time_diff_hours)) %>% 
  mutate(quick_three_edits = time_diff_hours + lag(time_diff_hours) + lag(time_diff_hours, 2) + lag(time_diff_hours, 3))

randoms_revision_history_page_details_ip_time_diffs_quick <- 
randoms_revision_history_page_details_ip_time_diffs %>% 
 nest(-page) %>%  
 mutate(edits_3 = map(data, ~.x %>% 
                              filter(less_than_one_week) %>% 
                              filter(n() > 4 ))) %>% 
  unnest(edits_3)

randoms_revision_history_page_details_ip_time_diffs %>% 
  bind_cols(rle_tbl2) %>% 
  filter(lths2 == 1) %>% 
  filter(time_diff_hours < 24) 
  
ggplot(randoms_revision_history_page_details_ip_time_diffs,
       aes(time_diff_days)) +
  geom_histogram()

```

time series of edits: breakouts relating to events? monthly seems like a good level of aggregation. What can we do for spike detection?
 
```{r}
randoms_revision_history_page_details_ip_location %>% 
  group_by(page) %>% 
  mutate(the_d = floor_date(rh_date, "day")) %>% 
    mutate(the_m = floor_date(rh_date, "month")) %>% 
  group_by(page, the_m) %>% 
  tally() %>% 
  filter(n() > 130) %>%   # explore this value
  ggplot() +
  geom_line(aes(the_m,
             n)) +
  facet_wrap( ~ page, scales = "free")
```

Spike detection

for randoms, what even is an anomaly? 

```{r}
# devtools::install_github("hrbrmstr/AnomalyDetection")
library(AnomalyDetection)
randoms_revision_history_page_details_ip_location_monthly <- 
randoms_revision_history_page_details_ip_location %>% 
  group_by(page) %>% 
  mutate(the_d = floor_date(rh_date, "day")) %>% 
    mutate(the_m = floor_date(rh_date, "month")) %>% 
  group_by(page, the_m) %>% 
  tally() %>% 
  filter(n() > 150) # explore this value

randoms_revision_history_page_details_ip_location_monthly_anom <- 
  randoms_revision_history_page_details_ip_location_monthly %>% 
  ungroup %>% 
  group_nest(page) %>% 
  mutate(anomalies = map(data, ~ad_ts(.x, 
                                      max_anoms = 0.2, 
                                      alpha = 0.005,
                                      threshold = "p95"))) %>% 
  unnest(anomalies)

ggplot() +
  geom_line(
    data=randoms_revision_history_page_details_ip_location_monthly %>% 
      filter(page %in% randoms_revision_history_page_details_ip_location_monthly_anom$page), 
    aes(the_m, n), 
    size=0.125, color="lightslategray")  +
  geom_point(
    data=randoms_revision_history_page_details_ip_location_monthly_anom, 
    aes(timestamp, anoms), 
    color="#cb181d", alpha=1/3) +
  scale_x_datetime(date_labels="%b\n%Y") +
  facet_wrap( ~ page, scales = "free_y")
  
```

for WH sites

```{r}

n <- 12 # months of edit data
revision_history_page_details_ip_addresses_df_monthly_edits <- 
revision_history_page_details_ip_addresses_df  %>% 
  mutate(the_d = floor_date(rh_date, "day")) %>% 
    mutate(the_m = floor_date(rh_date, "month")) %>% 
  group_by(Site, the_m) %>% 
  tally() %>% 
  filter(n() >  n ) # n months worth of data - explore this value

# distribution of edits per month
ggplot(revision_history_page_details_ip_addresses_df_monthly_edits,
       aes(n)) +
  geom_histogram() +
  scale_x_log10()

# subset so we get only sites with > n1 edits/month, and at least 3 months worth of edits

n1 <- 7  # edits per month
n2 <- 20 # months worth of edits
revision_history_page_details_ip_addresses_df_monthly_edits_10 <- 
revision_history_page_details_ip_addresses_df_monthly_edits %>% 
  filter(max(n) >= n1) %>% 
  group_by(Site) %>% 
  filter(n() >= n2) 

```



```{r}

# detect anomalies
revision_history_page_details_ip_addresses_df_monthly_edits_anoms <- 
  revision_history_page_details_ip_addresses_df_monthly_edits_10 %>% 
  ungroup %>% 
  group_nest(Site) %>% 
  mutate(anomalies = map(data, ~ad_ts(.x, 
                                      max_anoms = 0.2, 
                                      alpha = 0.005,
                                      threshold = "p95"))) %>% 
  unnest(anomalies) %>% 
  group_by(Site) %>% 
  filter(max(anoms) >= 10) # where the anomaly is n edits/month or more

ggplot() +
  geom_line(
    data=revision_history_page_details_ip_addresses_df_monthly_edits_10 %>% 
      filter(Site %in% revision_history_page_details_ip_addresses_df_monthly_edits_anoms$Site), 
    aes(the_m, n), 
    size=0.125, color="lightslategray")  +
  geom_point(
    data=revision_history_page_details_ip_addresses_df_monthly_edits_anoms, 
    aes(timestamp, anoms), 
    color="#cb181d", alpha=1/3) +
  scale_x_datetime(date_labels="%b\n%Y") +
  facet_wrap( ~ Site, scales = "free")
```




## Technicity

bots

What do bots do?

Do people undo bot actions often?


  - length of articles http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.451.4208&rep=rep1&type=pdf 
  - edit history to identify: diversity of contributions, 
  - reverts in edits for spotting contentious topics https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0038869 http://www.contropedia.net/  http://wwm.phy.bme.hu/ 
    - vandalism traces in edit history
  - talk page length
  - number of links in each page
  - page views: https://www.sciencedirect.com/science/article/abs/pii/S0006320716301240 
  - timing of edits: political events? Brexit?
    - citations in articles: https://arxiv.org/abs/0705.2106
  - are the citations scholarly? Archaeological? Meskell?
    - citations to these articles: https://www.tandfonline.com/doi/full/10.1080/0194262X.2016.1206052 
  - network analysis of editors, clusters of editors
  - sentiment analysis of talk pages
  - geolocation of anon editors by IP address
  - bots and technicity https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0171774 but see https://dl.acm.org/citation.cfm?id=3134684 
  
  - Compare to 10000 random Wikipedia sites
  - Compare to WH sites in media & academic literature
  